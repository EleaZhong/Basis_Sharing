# Basis Sharing Configuration for LLaMA 30B (20% compression)
model_args:
  model_type: "llama2"
  model_name: "jeffwan/llama-30b-hf"
  group_size: 2
  compression_ratio: 20
  context_length: 2048
  stride: 2048

calibration_args:
  dataset_name: "wikitext"
  calibration_size: 256
  calib_batch_size: 4
  dataset_cache_dir: null

model_saving:
  save_compressed_model: true
  compressed_model_path: "./compressed_models/llama_30b_20/wikitext"

lora_args:
  save_lora: true
  lora_r: 8
  lora_alpha: 32
  lora_output_dir: "./lora/llama_30b_20/wikitext"
  lora_train_batch_size: 1
  lora_learning_rate: 1.e-4
  lora_train_epoch: 2
