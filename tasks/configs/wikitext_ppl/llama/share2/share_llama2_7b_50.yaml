# Basis Sharing Configuration for LLaMA-2 7B (50% compression)
model_args:
  model_type: "llama2"
  model_name: "meta-llama/Llama-2-7b-hf"
  group_size: 2
  compression_ratio: 50
  context_length: 2048
  stride: 2048

calibration_args:
  dataset_name: "wikitext"
  calibration_size: 256
  calib_batch_size: 16
  dataset_cache_dir: null

model_saving:
  save_compressed_model: true
  compressed_model_path: "./compressed_models/llama2_7b_50/wikitext"

lora_args:
  save_lora: true
  lora_r: 8
  lora_alpha: 32
  lora_output_dir: "./lora/llama2_7b_50/wikitext"
  lora_train_batch_size: 1
  lora_learning_rate: 1.e-4
  lora_train_epoch: 2
