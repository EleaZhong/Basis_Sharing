{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea92b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d0d947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu129 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    }
   ],
   "source": [
    "from bbml.foundations.gpt2 import GPT2Foundation, GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65db2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "foundation = GPT2Foundation(  # based on nanoGPT\n",
    "    GPTConfig(),\n",
    "    None\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9aab61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the top, taps two aurora and un'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation.run(foundation.input_model(text=\"The quick brown fox jumps over\", max_new_tokens=10)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea72fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b75c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from enum import Enum\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class SplitLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, bias: bool, out_features: int | None = None, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.splits: nn.ModuleList = nn.ModuleList()\n",
    "        self.device=device\n",
    "        self.dtype=dtype\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.empty(out_features, device=device, dtype=dtype)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        outs = []\n",
    "        for split in self.splits:\n",
    "            outs.append(split(x))\n",
    "        out = torch.cat(outs, dim=-1)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ShareLinearState(str, Enum):\n",
    "    ORIGINAL = \"original\"  # Forward uses original weights\n",
    "    CALIBRATING = \"calibrating\"  # Forward uses original weights + tracks inputs\n",
    "    COMPRESSED = \"compressed\"  # Forward uses basis @ coefficient\n",
    "\n",
    "\n",
    "class ShareLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        basis_features: int,\n",
    "        out_features: int,\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.basis_features = basis_features\n",
    "        self.out_features = out_features\n",
    "        self.device=device\n",
    "        self.dtype=dtype\n",
    "        \n",
    "        self.basis = nn.Parameter(\n",
    "            torch.empty(basis_features, in_features, device=device, dtype=dtype)\n",
    "        )\n",
    "        self.coefficient = nn.Parameter(\n",
    "            torch.empty(out_features, basis_features, device=device, dtype=dtype)\n",
    "        )\n",
    "        self.original = nn.Parameter(\n",
    "            torch.empty(out_features, in_features, device=device, dtype=dtype)\n",
    "        )\n",
    "        self.state = ShareLinearState.ORIGINAL\n",
    "        self.register_buffer(\"xtx\", None)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def track_input(self, x: torch.Tensor):\n",
    "\n",
    "        inp = x.detach().float()\n",
    "        inp = rearrange(inp, \"B T H -> (B T) H\")\n",
    "        xtx = inp.T @ inp  # (H, H)\n",
    "\n",
    "        if self.xtx is None:\n",
    "            self.xtx = torch.zeros_like(xtx)\n",
    "\n",
    "        self.xtx = self.xtx + xtx  # sum over iters xTx same as XTX\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.state == ShareLinearState.CALIBRATING:\n",
    "            self.track_input(x)\n",
    "\n",
    "        if self.state != ShareLinearState.COMPRESSED:\n",
    "            return F.linear(x, self.original)\n",
    "\n",
    "        b = F.linear(x, self.basis)\n",
    "        out = F.linear(b, self.coefficient)\n",
    "        return out\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in={self.in_features}, basis={self.basis_features}, out={self.out_features}, state={self.state.value}\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18ee421",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((10, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948a91fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a.T @ a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a3cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "import bbml\n",
    "\n",
    "class WeightConfig(BaseModel):\n",
    "    pattern: str\n",
    "    split: bool|Literal[\"qkv\", \"heads\"] = False\n",
    "    qkv_order: str = \"qkv\"\n",
    "    n_head: int|None = None\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def qkv_string(self):\n",
    "        if self.split:\n",
    "            if not self.qkv_order or set(self.qkv_order.lower()) != {\"q\", \"k\", \"v\"}:\n",
    "                raise ValueError(\"qkv_order must contain exactly 'q', 'k', and 'v' characters when split='qkv'\")\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_heads_split(self):\n",
    "        if self.split == \"heads\" and self.n_head is None:\n",
    "            raise ValueError(\"n_head must be set when split='heads'\")\n",
    "        return self\n",
    "\n",
    "\n",
    "class SplitConfig(BaseModel):\n",
    "    weight_types: dict[str, WeightConfig]\n",
    "    group_size: int\n",
    "    layer_pattern: str\n",
    "    compression_rate: float = Field(ge=0)  # allow >1 rate, meaning size increase\n",
    "\n",
    "\n",
    "class SplitLinearFinetuner(bbml.Finetuner):  # using finetuner interface for now\n",
    "    def __init__(self, model: bbml.Foundation, config: SplitConfig):\n",
    "        super().__init__(model)\n",
    "        self.config: SplitConfig = config\n",
    "    \n",
    "        self.original_weights = []\n",
    "\n",
    "        named_modules = {k:v for k,v in model.named_modules()}\n",
    "\n",
    "        for name, module in named_modules.items():\n",
    "            for wtype, wtype_cfg in self.config.weight_types.items():\n",
    "                if re.match(wtype_cfg.pattern, name) is not None:\n",
    "                    name_parts = name.split(\".\")\n",
    "                    parent_name = \".\".join(name_parts[:-1])\n",
    "                    parent = named_modules[parent_name]\n",
    "                    list_id = None\n",
    "                    if name_parts[-1].isdigit():\n",
    "                        list_id = int(name_parts[-1])\n",
    "                    \n",
    "                    layer_num = re.search(config.layer_pattern, name).group(1)\n",
    "\n",
    "                    self.original_weights.append({\n",
    "                        \"name\": name,\n",
    "                        \"module\": module,\n",
    "                        \"parent\": parent,\n",
    "                        \"list_id\": list_id,\n",
    "                        \"last_name_part\": name_parts[-1],\n",
    "                        \"layer\": layer_num,\n",
    "                        \"weight_type\": wtype,\n",
    "                    })\n",
    "\n",
    "    def get_train_parameters(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def save(self, save_path: str | Path):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def load(self, load_path: str | Path):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def split_weights(self):\n",
    "\n",
    "        self.all_sharelinears = {}\n",
    "        self.weight_types = defaultdict(list)\n",
    "\n",
    "        for wt_dict in self.original_weights:\n",
    "            wtype = wt_dict[\"weight_type\"]\n",
    "            wtype_cfg = self.config.weight_types[wtype]\n",
    "            module = wt_dict[\"module\"]\n",
    "            name = wt_dict[\"name\"]\n",
    "            has_bias = module.bias is not None\n",
    "            in_feats = module.in_features\n",
    "            out_feats = module.out_features\n",
    "            split_linear = SplitLinear(\n",
    "                bias=has_bias,\n",
    "                out_features=out_feats,\n",
    "                dtype=self.model.dtype,\n",
    "                device=self.model.device,\n",
    "            )\n",
    "            if has_bias:\n",
    "                split_linear.bias.data = module.bias.data\n",
    "            if wt_dict[\"list_id\"] is None:\n",
    "                setattr(wt_dict[\"parent\"], wt_dict[\"last_name_part\"], split_linear)\n",
    "            else:\n",
    "                idx = wt_dict[\"list_id\"]\n",
    "                wt_dict[\"parent\"][idx] = split_linear\n",
    "\n",
    "            if wtype_cfg.split == \"qkv\" or wtype_cfg.split == \"heads\":\n",
    "                assert out_feats % 3 == 0\n",
    "                qkv_feats = out_feats // 3 \n",
    "            \n",
    "            if wtype_cfg.split == \"qkv\":\n",
    "                cur_ind = 0            \n",
    "                for qkv_part in wtype_cfg.qkv_order:\n",
    "                    to_part = ShareLinear(\n",
    "                        in_features=in_feats,\n",
    "                        basis_features=in_feats,\n",
    "                        out_features=qkv_feats,\n",
    "                        dtype=self.model.dtype,\n",
    "                        device=self.model.device,\n",
    "                    )    \n",
    "                    to_part.original.data = module.weight.data[cur_ind:cur_ind+qkv_feats,:]\n",
    "                    cur_ind += qkv_feats\n",
    "                    \n",
    "                    split_linear.splits.append(to_part)\n",
    "                    self.all_sharelinears[f\"{name}.{qkv_part}\"] = to_part\n",
    "                    \n",
    "                    split_wtype = f\"{wtype}.{qkv_part}\"\n",
    "                    self.weight_types[split_wtype].append({\n",
    "                        \"module\": to_part,\n",
    "                        \"name\": f\"{name}.{qkv_part}\",\n",
    "                        \"split_name\": qkv_part,\n",
    "                        \"weight_type\": wtype,\n",
    "                        \"layer\": wt_dict[\"layer\"],\n",
    "                    })\n",
    "                \n",
    "            elif wtype_cfg.split == \"heads\":\n",
    "                n_heads = wtype_cfg.n_head\n",
    "                head_dim = qkv_feats // n_heads\n",
    "\n",
    "                cur_ind = 0\n",
    "                for qkv_part in wtype_cfg.qkv_order:\n",
    "                    for head_num in range(n_heads):\n",
    "                        to_head = ShareLinear(\n",
    "                            in_features=in_feats,\n",
    "                            basis_features=in_feats,\n",
    "                            out_features=head_dim,\n",
    "                            dtype=self.model.dtype,\n",
    "                            device=self.model.device,\n",
    "                        )\n",
    "                        to_head.original.data = module.weight.data[cur_ind:cur_ind+head_dim,:]  # [out_dim, in_dim] -> [head_dim, in_dim]\n",
    "                        cur_ind += head_dim\n",
    "\n",
    "                        split_linear.splits.append(to_head)\n",
    "                        self.all_sharelinears[f\"{name}.{qkv_part}.{head_num}\"] = to_head\n",
    "                        \n",
    "                        split_wtype = f\"{wtype}.{qkv_part}.{head_num}\"\n",
    "                        self.weight_types[split_wtype].append({\n",
    "                            \"module\": to_head,\n",
    "                            \"name\": f\"{name}.{qkv_part}.{head_num}\",\n",
    "                            \"split_name\": f\"{qkv_part}.{head_num}\",\n",
    "                            \"weight_type\": wtype,\n",
    "                            \"layer\": wt_dict[\"layer\"],\n",
    "                        })\n",
    "                \n",
    "\n",
    "            else: # no split\n",
    "                in_linear = ShareLinear(\n",
    "                    in_features=in_feats,\n",
    "                    basis_features=in_feats,\n",
    "                    out_features=out_feats,\n",
    "                    dtype=self.model.dtype,\n",
    "                    device=self.model.device,\n",
    "                )\n",
    "                in_linear.original.data = module.weight.data\n",
    "\n",
    "                split_linear.splits.append(in_linear)\n",
    "                self.all_sharelinears[name] = in_linear\n",
    "                self.weight_types[wtype].append({\n",
    "                    \"module\": in_linear,\n",
    "                    \"name\": name,\n",
    "                    \"split_name\": \"\",\n",
    "                    \"weight_type\": wtype,\n",
    "                    \"layer\": wt_dict[\"layer\"],\n",
    "                })\n",
    "        \n",
    "    def group_weights(self):\n",
    "        self.groups = defaultdict(list)\n",
    "        \n",
    "        group_size = self.config.group_size\n",
    "        for weight_type, weights_list in self.weight_types.items():\n",
    "            # group_adjacent\n",
    "            cur_group = []\n",
    "            for weights_dict in weights_list:\n",
    "                cur_group.append(weights_dict)\n",
    "                if len(cur_group) >= group_size:\n",
    "                    self.groups[weight_type].append(cur_group)\n",
    "                    cur_group = []\n",
    "            if len(cur_group) > 0:  # leftovers\n",
    "                self.groups[weight_type].append(cur_group)\n",
    "                cur_group = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calibrate(self, dataset: Dataset|bbml.DataPipe):\n",
    "        self.model.eval()\n",
    "        for l in self.all_sharelinears.values():\n",
    "            l.state = ShareLinearState.CALIBRATING\n",
    "        \n",
    "        if not isinstance(dataset, bbml.DataPipe):\n",
    "            dataset = bbml.DataPipe(\n",
    "                batch_size=1,\n",
    "                shuffle=False,\n",
    "                num_workers=2,\n",
    "            ).add_dataset(\n",
    "                dataset\n",
    "            ).add_transforms(\n",
    "                self.model.data_transforms\n",
    "            )\n",
    "\n",
    "        dataloader = dataset.get_loader()\n",
    "\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            step_info = {\n",
    "                \"step\": 0,\n",
    "                \"split\": \"validation\",\n",
    "            }\n",
    "            batch.update(step_info)\n",
    "            self.model.single_step(batch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_num_basis(\n",
    "        in_features: int, out_features: int, compression_ratio: int\n",
    "    ) -> int:\n",
    "        total_original = in_features * out_features\n",
    "        num_basis = (total_original * compression_ratio) / (in_features + out_features)\n",
    "        return max(1, int(num_basis))\n",
    "\n",
    "    def run_svd(self):\n",
    "        self.group_bases = defaultdict(list)\n",
    "        for weight_type, groups_list in self.groups.items():\n",
    "            for group in tqdm.tqdm(groups_list):                \n",
    "                all_xtx = []\n",
    "                all_weights = []\n",
    "                out_sizes = []\n",
    "                for weights_dict in group:\n",
    "                    all_xtx.append(weights_dict[\"module\"].xtx)\n",
    "                    all_weights.append(weights_dict[\"module\"].original.data)\n",
    "                    out_sizes.append(weights_dict[\"module\"].original.data.size(0))\n",
    "                    \n",
    "                all_xtx = sum(all_xtx)\n",
    "                try:\n",
    "                    S = torch.linalg.cholesky(all_xtx).T\n",
    "                except Exception as e:\n",
    "                    print(\"Warning: eigen scaling_diag_matrix is not positive!\")\n",
    "                    eigenvalues = torch.linalg.eigvalsh(all_xtx)\n",
    "                    all_xtx += (- eigenvalues[0] + 7e-6) * torch.eye(all_xtx.shape[0]).to(all_xtx.device)\n",
    "                    S = torch.linalg.cholesky(all_xtx).T\n",
    "                S_inv = torch.linalg.inv(S)\n",
    "                W = torch.cat(all_weights, dim=0).T  # -> [in, out_cat]\n",
    "                \n",
    "\n",
    "                W_white = S @ W\n",
    "\n",
    "                U, sigma, Vh = torch.linalg.svd(W_white, full_matrices=False)  # different function torch.svd default some=True is full_matrices=False\n",
    "\n",
    "                total_basis = S_inv @ U @ torch.diag(sigma)\n",
    "                total_coefficient = Vh\n",
    "                k = self.compute_num_basis(W.size(0), W.size(1), self.config.compression_rate)\n",
    "                compressed_basis = total_basis[:, :k].to(dtype=self.model.dtype)\n",
    "                compressed_coefficient = total_coefficient[:k, :].to(dtype=self.model.dtype)\n",
    "\n",
    "\n",
    "                group_basis = nn.Parameter(compressed_basis.T)  # [out,in] / [num_basis, in_feat]\n",
    "                self.group_bases[weight_type].append(group_basis)\n",
    "                \n",
    "                cur_id = 0\n",
    "                for i, weights_dict in enumerate(group):\n",
    "                    module = weights_dict[\"module\"]\n",
    "                    module.basis = group_basis\n",
    "                    cur_out_dim = out_sizes[i]\n",
    "                    module.coefficient.data = compressed_coefficient[:,cur_id:cur_id+cur_out_dim].T  # [out,in] / [out_feat,num_basis]\n",
    "                    cur_id += cur_out_dim\n",
    "                    module.state = ShareLinearState.COMPRESSED\n",
    "                    module.basis_features = k\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "691bae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "weight_types:\n",
    "    attn_c_attn:\n",
    "        pattern: '.*\\.attn\\.c_attn$'\n",
    "        split: qkv  # qkv, heads\n",
    "        qkv_order: qkv  # string\n",
    "        n_head: 12\n",
    "    attn_c_proj: \n",
    "        pattern: '.*\\.attn\\.c_proj$'\n",
    "    mlp_c_fc: \n",
    "        pattern: '.*\\.mlp\\.c_fc$'\n",
    "    mlp_c_proj: \n",
    "        pattern: '.*\\.mlp\\.c_proj$'\n",
    "\n",
    "layer_pattern: 'h\\.(\\d+)'\n",
    "\n",
    "group_size: 2\n",
    "compression_rate: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfeb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8286bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec166d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "facf4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitcfg = SplitConfig(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a2521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0467a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = SplitLinearFinetuner(foundation, splitcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a2edd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.split_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e7c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.group_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bb726b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:07<00:00, 71.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# train_dp = DataPipe(\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=16,\n",
    "# ).add_dataset(\n",
    "#     WikiTextDataset(split=\"train\")\n",
    "# ).add_transforms(\n",
    "#     gpt.data_transforms\n",
    "# )\n",
    "from bbml.data.datasets import WikiTextDataset\n",
    "\n",
    "ds = WikiTextDataset(split=\"train\")\n",
    "ds.ds = ds.ds.select(range(512))\n",
    "\n",
    "wrapper.calibrate(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f983c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 14.10it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 17.54it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 17.56it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 17.54it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 15.95it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.91it/s]\n"
     ]
    }
   ],
   "source": [
    "wrapper.run_svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddd9b682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over him and begins moving to possibly an approaching dog.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation.run(foundation.input_model(text=\"The quick brown fox jumps over\", max_new_tokens=10)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "626b552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
      "[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "100%|██████████| 62/62 [00:00<00:00, 710.62it/s]\n",
      "Full-text loglikelihood: 100%|██████████| 62/62 [00:07<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "wikitext:\n",
      "  alias: wikitext\n",
      "  word_perplexity,none: 97.0250\n",
      "  word_perplexity_stderr,none: N/A\n",
      "  byte_perplexity,none: 2.3526\n",
      "  byte_perplexity_stderr,none: N/A\n",
      "  bits_per_byte,none: 1.2343\n",
      "  bits_per_byte_stderr,none: N/A\n"
     ]
    }
   ],
   "source": [
    "from bbml.foundations.gpt2.evaluation import GPT2FoundationLM\n",
    "\n",
    "from lm_eval.evaluator import simple_evaluate\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "lm = GPT2FoundationLM(foundation)\n",
    "\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=lm,\n",
    "    tasks=[\"wikitext\"],\n",
    "    batch_size=8,\n",
    "    device=foundation.device,\n",
    "    log_samples=False,\n",
    ")\n",
    "\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "word_perplexity = None\n",
    "if \"results\" in results:\n",
    "    for task, metrics in results[\"results\"].items():\n",
    "        print(f\"\\n{task}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric_name}: {value:.4f}\")\n",
    "                if \"word_perplexity\" in metric_name:\n",
    "                    word_perplexity = value\n",
    "            else:\n",
    "                print(f\"  {metric_name}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc513b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c30897b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.transformer.h.0.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.0.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.0.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.0.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.0.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.0.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.1.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.1.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.1.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.1.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.1.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.1.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.2.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.2.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.2.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.2.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.2.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.2.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.3.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.3.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.3.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.3.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.3.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.3.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.4.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.4.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.4.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.4.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.4.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.4.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.5.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.5.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.5.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.5.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.5.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.5.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.6.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.6.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.6.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.6.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.6.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.6.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.7.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.7.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.7.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.7.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.7.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.7.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.8.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.8.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.8.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.8.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.8.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.8.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.9.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.9.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.9.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.9.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.9.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.9.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.10.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.10.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.10.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.10.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.10.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.10.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       " 'model.transformer.h.11.attn.c_attn.q': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.11.attn.c_attn.k': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.11.attn.c_attn.v': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.11.attn.c_proj': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       " 'model.transformer.h.11.mlp.c_fc': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       " 'model.transformer.h.11.mlp.c_proj': ShareLinear(in=3072, basis=1024, out=768, state=compressed)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.all_sharelinears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd89f808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'attn_c_attn.q': [{'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.0.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '0'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.1.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '1'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.2.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '2'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.3.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '3'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.4.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '4'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.5.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '5'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.6.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '6'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.7.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '7'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.8.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '8'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.9.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '9'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.10.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '10'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.11.attn.c_attn.q',\n",
       "               'split_name': 'q',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '11'}],\n",
       "             'attn_c_attn.k': [{'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.0.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '0'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.1.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '1'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.2.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '2'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.3.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '3'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.4.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '4'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.5.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '5'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.6.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '6'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.7.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '7'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.8.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '8'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.9.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '9'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.10.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '10'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.11.attn.c_attn.k',\n",
       "               'split_name': 'k',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '11'}],\n",
       "             'attn_c_attn.v': [{'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.0.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '0'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.1.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '1'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.2.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '2'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.3.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '3'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.4.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '4'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.5.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '5'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.6.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '6'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.7.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '7'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.8.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '8'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.9.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '9'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.10.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '10'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.11.attn.c_attn.v',\n",
       "               'split_name': 'v',\n",
       "               'weight_type': 'attn_c_attn',\n",
       "               'layer': '11'}],\n",
       "             'attn_c_proj': [{'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.0.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '0'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.1.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '1'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.2.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '2'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.3.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '3'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.4.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '4'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.5.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '5'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.6.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '6'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.7.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '7'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.8.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '8'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.9.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '9'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.10.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '10'},\n",
       "              {'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.11.attn.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'attn_c_proj',\n",
       "               'layer': '11'}],\n",
       "             'mlp_c_fc': [{'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.0.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '0'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.1.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '1'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.2.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '2'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.3.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '3'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.4.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '4'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.5.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '5'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.6.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '6'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.7.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '7'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.8.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '8'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.9.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '9'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.10.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '10'},\n",
       "              {'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
       "               'name': 'model.transformer.h.11.mlp.c_fc',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_fc',\n",
       "               'layer': '11'}],\n",
       "             'mlp_c_proj': [{'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.0.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '0'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.1.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '1'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.2.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '2'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.3.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '3'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.4.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '4'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.5.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '5'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.6.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '6'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.7.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '7'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.8.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '8'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.9.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '9'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.10.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '10'},\n",
       "              {'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
       "               'name': 'model.transformer.h.11.mlp.c_proj',\n",
       "               'split_name': '',\n",
       "               'weight_type': 'mlp_c_proj',\n",
       "               'layer': '11'}]})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.weight_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd779363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',\n",
       "  GPT2Foundation(\n",
       "    (model): GPT(\n",
       "      (transformer): ModuleDict(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x Block(\n",
       "            (ln_1): LayerNorm()\n",
       "            (attn): CausalSelfAttention(\n",
       "              (c_attn): SplitLinear(\n",
       "                (splits): ModuleList(\n",
       "                  (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "                )\n",
       "              )\n",
       "              (c_proj): SplitLinear(\n",
       "                (splits): ModuleList(\n",
       "                  (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "                )\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm()\n",
       "            (mlp): MLP(\n",
       "              (c_fc): SplitLinear(\n",
       "                (splits): ModuleList(\n",
       "                  (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "                )\n",
       "              )\n",
       "              (gelu): GELU(approximate='none')\n",
       "              (c_proj): SplitLinear(\n",
       "                (splits): ModuleList(\n",
       "                  (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )),\n",
       " ('model',\n",
       "  GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): SplitLinear(\n",
       "              (splits): ModuleList(\n",
       "                (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "              )\n",
       "            )\n",
       "            (c_proj): SplitLinear(\n",
       "              (splits): ModuleList(\n",
       "                (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "              )\n",
       "            )\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): SplitLinear(\n",
       "              (splits): ModuleList(\n",
       "                (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "              )\n",
       "            )\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): SplitLinear(\n",
       "              (splits): ModuleList(\n",
       "                (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "              )\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )),\n",
       " ('model.transformer',\n",
       "  ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): SplitLinear(\n",
       "            (splits): ModuleList(\n",
       "              (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "            )\n",
       "          )\n",
       "          (c_proj): SplitLinear(\n",
       "            (splits): ModuleList(\n",
       "              (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "            )\n",
       "          )\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): SplitLinear(\n",
       "            (splits): ModuleList(\n",
       "              (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "            )\n",
       "          )\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): SplitLinear(\n",
       "            (splits): ModuleList(\n",
       "              (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "            )\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )),\n",
       " ('model.transformer.wte', Embedding(50257, 768)),\n",
       " ('model.transformer.wpe', Embedding(1024, 768)),\n",
       " ('model.transformer.drop', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h',\n",
       "  ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (ln_1): LayerNorm()\n",
       "      (attn): CausalSelfAttention(\n",
       "        (c_attn): SplitLinear(\n",
       "          (splits): ModuleList(\n",
       "            (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "          )\n",
       "        )\n",
       "        (c_proj): SplitLinear(\n",
       "          (splits): ModuleList(\n",
       "            (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "          )\n",
       "        )\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (c_fc): SplitLinear(\n",
       "          (splits): ModuleList(\n",
       "            (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "          )\n",
       "        )\n",
       "        (gelu): GELU(approximate='none')\n",
       "        (c_proj): SplitLinear(\n",
       "          (splits): ModuleList(\n",
       "            (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.0',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.0.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.0.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.0.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.0.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.0.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.0.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.0.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.0.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.0.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.0.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.0.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.0.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.0.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.0.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.0.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.0.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.0.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.0.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.0.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.0.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.0.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.0.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.1',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.1.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.1.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.1.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.1.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.1.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.1.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.1.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.1.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.1.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.1.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.1.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.1.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.1.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.1.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.1.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.1.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.1.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.1.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.1.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.1.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.1.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.1.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.2',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.2.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.2.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.2.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.2.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.2.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.2.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.2.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.2.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.2.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.2.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.2.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.2.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.2.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.2.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.2.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.2.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.2.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.2.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.2.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.2.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.2.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.2.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.3',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.3.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.3.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.3.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.3.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.3.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.3.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.3.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.3.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.3.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.3.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.3.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.3.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.3.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.3.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.3.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.3.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.3.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.3.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.3.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.3.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.3.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.3.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.4',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.4.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.4.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.4.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.4.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.4.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.4.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.4.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.4.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.4.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.4.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.4.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.4.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.4.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.4.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.4.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.4.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.4.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.4.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.4.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.4.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.4.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.4.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.5',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.5.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.5.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.5.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.5.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.5.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.5.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.5.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.5.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.5.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.5.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.5.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.5.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.5.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.5.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.5.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.5.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.5.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.5.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.5.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.5.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.5.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.5.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.6',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.6.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.6.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.6.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.6.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.6.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.6.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.6.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.6.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.6.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.6.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.6.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.6.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.6.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.6.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.6.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.6.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.6.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.6.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.6.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.6.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.6.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.6.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.7',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.7.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.7.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.7.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.7.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.7.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.7.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.7.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.7.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.7.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.7.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.7.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.7.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.7.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.7.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.7.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.7.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.7.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.7.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.7.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.7.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.7.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.7.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.8',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.8.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.8.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.8.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.8.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.8.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.8.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.8.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.8.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.8.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.8.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.8.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.8.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.8.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.8.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.8.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.8.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.8.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.8.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.8.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.8.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.8.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.8.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.9',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.9.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.9.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.9.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.9.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.9.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.9.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.9.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.9.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.9.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.9.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.9.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.9.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.9.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.9.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.9.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.9.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.9.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.9.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.9.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.9.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.9.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.9.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.10',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.10.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.10.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.10.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.10.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.10.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.10.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.10.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.10.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.10.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.10.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.10.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.10.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.10.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.10.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.10.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.10.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.10.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.10.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.10.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.10.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.10.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.10.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.11',\n",
       "  Block(\n",
       "    (ln_1): LayerNorm()\n",
       "    (attn): CausalSelfAttention(\n",
       "      (c_attn): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ln_2): LayerNorm()\n",
       "    (mlp): MLP(\n",
       "      (c_fc): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (gelu): GELU(approximate='none')\n",
       "      (c_proj): SplitLinear(\n",
       "        (splits): ModuleList(\n",
       "          (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.11.ln_1', LayerNorm()),\n",
       " ('model.transformer.h.11.attn',\n",
       "  CausalSelfAttention(\n",
       "    (c_attn): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.11.attn.c_attn',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.11.attn.c_attn.splits',\n",
       "  ModuleList(\n",
       "    (0-2): 3 x ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.11.attn.c_attn.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.11.attn.c_attn.splits.1',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.11.attn.c_attn.splits.2',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.11.attn.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.11.attn.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=512, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.11.attn.c_proj.splits.0',\n",
       "  ShareLinear(in=768, basis=512, out=768, state=compressed)),\n",
       " ('model.transformer.h.11.attn.attn_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.11.attn.resid_dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.h.11.ln_2', LayerNorm()),\n",
       " ('model.transformer.h.11.mlp',\n",
       "  MLP(\n",
       "    (c_fc): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (c_proj): SplitLinear(\n",
       "      (splits): ModuleList(\n",
       "        (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )),\n",
       " ('model.transformer.h.11.mlp.c_fc',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.11.mlp.c_fc.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=768, basis=682, out=3072, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.11.mlp.c_fc.splits.0',\n",
       "  ShareLinear(in=768, basis=682, out=3072, state=compressed)),\n",
       " ('model.transformer.h.11.mlp.gelu', GELU(approximate='none')),\n",
       " ('model.transformer.h.11.mlp.c_proj',\n",
       "  SplitLinear(\n",
       "    (splits): ModuleList(\n",
       "      (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "    )\n",
       "  )),\n",
       " ('model.transformer.h.11.mlp.c_proj.splits',\n",
       "  ModuleList(\n",
       "    (0): ShareLinear(in=3072, basis=1024, out=768, state=compressed)\n",
       "  )),\n",
       " ('model.transformer.h.11.mlp.c_proj.splits.0',\n",
       "  ShareLinear(in=3072, basis=1024, out=768, state=compressed)),\n",
       " ('model.transformer.h.11.mlp.dropout', Dropout(p=0.0, inplace=False)),\n",
       " ('model.transformer.ln_f', LayerNorm()),\n",
       " ('model.lm_head', Linear(in_features=768, out_features=50257, bias=False))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(foundation.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a663bbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'attn_c_attn.k': [[{'layer': '0',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.0.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '1',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.1.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '2',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.2.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '3',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.3.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '4',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.4.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '5',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.5.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '6',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.6.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '7',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.7.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '8',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.8.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '9',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.9.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '10',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.10.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '11',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.11.attn.c_attn.k',\n",
      "                                 'split_name': 'k',\n",
      "                                 'weight_type': 'attn_c_attn'}]],\n",
      "             'attn_c_attn.q': [[{'layer': '0',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.0.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '1',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.1.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '2',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.2.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '3',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.3.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '4',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.4.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '5',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.5.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '6',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.6.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '7',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.7.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '8',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.8.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '9',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.9.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '10',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.10.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '11',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.11.attn.c_attn.q',\n",
      "                                 'split_name': 'q',\n",
      "                                 'weight_type': 'attn_c_attn'}]],\n",
      "             'attn_c_attn.v': [[{'layer': '0',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.0.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '1',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.1.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '2',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.2.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '3',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.3.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '4',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.4.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '5',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.5.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '6',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.6.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '7',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.7.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '8',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.8.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '9',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.9.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'}],\n",
      "                               [{'layer': '10',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.10.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'},\n",
      "                                {'layer': '11',\n",
      "                                 'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                 'name': 'model.transformer.h.11.attn.c_attn.v',\n",
      "                                 'split_name': 'v',\n",
      "                                 'weight_type': 'attn_c_attn'}]],\n",
      "             'attn_c_proj': [[{'layer': '0',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.0.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'},\n",
      "                              {'layer': '1',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.1.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'}],\n",
      "                             [{'layer': '2',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.2.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'},\n",
      "                              {'layer': '3',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.3.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'}],\n",
      "                             [{'layer': '4',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.4.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'},\n",
      "                              {'layer': '5',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.5.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'}],\n",
      "                             [{'layer': '6',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.6.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'},\n",
      "                              {'layer': '7',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.7.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'}],\n",
      "                             [{'layer': '8',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.8.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'},\n",
      "                              {'layer': '9',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.9.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'}],\n",
      "                             [{'layer': '10',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.10.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'},\n",
      "                              {'layer': '11',\n",
      "                               'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                               'name': 'model.transformer.h.11.attn.c_proj',\n",
      "                               'split_name': '',\n",
      "                               'weight_type': 'attn_c_proj'}]],\n",
      "             'mlp_c_fc': [[{'layer': '0',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.0.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'},\n",
      "                           {'layer': '1',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.1.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'}],\n",
      "                          [{'layer': '2',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.2.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'},\n",
      "                           {'layer': '3',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.3.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'}],\n",
      "                          [{'layer': '4',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.4.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'},\n",
      "                           {'layer': '5',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.5.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'}],\n",
      "                          [{'layer': '6',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.6.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'},\n",
      "                           {'layer': '7',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.7.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'}],\n",
      "                          [{'layer': '8',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.8.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'},\n",
      "                           {'layer': '9',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.9.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'}],\n",
      "                          [{'layer': '10',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.10.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'},\n",
      "                           {'layer': '11',\n",
      "                            'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                            'name': 'model.transformer.h.11.mlp.c_fc',\n",
      "                            'split_name': '',\n",
      "                            'weight_type': 'mlp_c_fc'}]],\n",
      "             'mlp_c_proj': [[{'layer': '0',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.0.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'},\n",
      "                             {'layer': '1',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.1.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'}],\n",
      "                            [{'layer': '2',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.2.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'},\n",
      "                             {'layer': '3',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.3.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'}],\n",
      "                            [{'layer': '4',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.4.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'},\n",
      "                             {'layer': '5',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.5.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'}],\n",
      "                            [{'layer': '6',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.6.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'},\n",
      "                             {'layer': '7',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.7.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'}],\n",
      "                            [{'layer': '8',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.8.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'},\n",
      "                             {'layer': '9',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.9.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'}],\n",
      "                            [{'layer': '10',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.10.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'},\n",
      "                             {'layer': '11',\n",
      "                              'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.11.mlp.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'mlp_c_proj'}]]})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(wrapper.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f192f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'attn_c_attn.k': [{'layer': '0',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.0.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '1',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.1.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '2',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.2.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '3',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.3.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '4',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.4.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '5',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.5.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '6',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.6.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '7',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.7.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '8',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.8.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '9',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.9.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '10',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.10.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '11',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.11.attn.c_attn.k',\n",
      "                                'split_name': 'k',\n",
      "                                'weight_type': 'attn_c_attn'}],\n",
      "             'attn_c_attn.q': [{'layer': '0',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.0.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '1',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.1.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '2',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.2.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '3',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.3.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '4',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.4.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '5',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.5.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '6',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.6.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '7',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.7.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '8',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.8.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '9',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.9.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '10',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.10.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '11',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.11.attn.c_attn.q',\n",
      "                                'split_name': 'q',\n",
      "                                'weight_type': 'attn_c_attn'}],\n",
      "             'attn_c_attn.v': [{'layer': '0',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.0.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '1',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.1.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '2',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.2.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '3',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.3.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '4',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.4.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '5',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.5.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '6',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.6.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '7',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.7.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '8',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.8.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '9',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.9.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '10',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.10.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'},\n",
      "                               {'layer': '11',\n",
      "                                'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                                'name': 'model.transformer.h.11.attn.c_attn.v',\n",
      "                                'split_name': 'v',\n",
      "                                'weight_type': 'attn_c_attn'}],\n",
      "             'attn_c_proj': [{'layer': '0',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.0.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '1',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.1.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '2',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.2.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '3',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.3.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '4',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.4.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '5',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.5.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '6',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.6.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '7',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.7.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '8',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.8.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '9',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.9.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '10',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.10.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'},\n",
      "                             {'layer': '11',\n",
      "                              'module': ShareLinear(in=768, basis=512, out=768, state=compressed),\n",
      "                              'name': 'model.transformer.h.11.attn.c_proj',\n",
      "                              'split_name': '',\n",
      "                              'weight_type': 'attn_c_proj'}],\n",
      "             'mlp_c_fc': [{'layer': '0',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.0.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '1',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.1.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '2',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.2.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '3',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.3.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '4',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.4.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '5',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.5.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '6',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.6.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '7',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.7.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '8',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.8.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '9',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.9.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '10',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.10.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'},\n",
      "                          {'layer': '11',\n",
      "                           'module': ShareLinear(in=768, basis=682, out=3072, state=compressed),\n",
      "                           'name': 'model.transformer.h.11.mlp.c_fc',\n",
      "                           'split_name': '',\n",
      "                           'weight_type': 'mlp_c_fc'}],\n",
      "             'mlp_c_proj': [{'layer': '0',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.0.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '1',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.1.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '2',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.2.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '3',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.3.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '4',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.4.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '5',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.5.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '6',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.6.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '7',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.7.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '8',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.8.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '9',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.9.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '10',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.10.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'},\n",
      "                            {'layer': '11',\n",
      "                             'module': ShareLinear(in=3072, basis=1024, out=768, state=compressed),\n",
      "                             'name': 'model.transformer.h.11.mlp.c_proj',\n",
      "                             'split_name': '',\n",
      "                             'weight_type': 'mlp_c_proj'}]})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pprint(wrapper.weight_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb2cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ae461d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 682])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.all_sharelinears[\"model.transformer.h.11.mlp.c_fc\"].coefficient.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249313e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
