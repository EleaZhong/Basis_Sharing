{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea92b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d0d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbml.foundations.gpt2 import GPT2Foundation, GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65db2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "foundation = GPT2Foundation(  # based on nanoGPT\n",
    "    GPTConfig(),\n",
    "    None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9aab61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the front wheel of a Joltcar. Then'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foundation.run(foundation.input_model(text=\"The quick brown fox jumps over\", max_new_tokens=10)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea72fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b75c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from enum import Enum\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class SplitLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, bias: bool, out_features: int | None = None, device=None, dtype=None):\n",
    "        super().__init__()\n",
    "        self.splits: nn.ModuleList = nn.ModuleList()\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.empty(out_features, device=device, dtype=dtype)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        outs = []\n",
    "        for split in self.splits:\n",
    "            outs.append(split(x))\n",
    "        out = torch.cat(outs, dim=-1)\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ShareLinearState(str, Enum):\n",
    "    ORIGINAL = \"original\"  # Forward uses original weights\n",
    "    CALIBRATING = \"calibrating\"  # Forward uses original weights + tracks inputs\n",
    "    COMPRESSED = \"compressed\"  # Forward uses basis @ coefficient\n",
    "\n",
    "\n",
    "class ShareLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        basis_features: int,\n",
    "        out_features: int,\n",
    "        device=None,\n",
    "        dtype=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.basis_features = basis_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.basis = nn.Parameter(\n",
    "            torch.empty(basis_features, in_features, device=device, dtype=dtype)\n",
    "        )\n",
    "        self.coefficient = nn.Parameter(\n",
    "            torch.empty(out_features, basis_features, device=device, dtype=dtype)\n",
    "        )\n",
    "        self.original = nn.Parameter(\n",
    "            torch.empty(out_features, in_features, device=device, dtype=dtype)\n",
    "        )\n",
    "        self.state = ShareLinearState.ORIGINAL\n",
    "        self.register_buffer(\"xtx\", None)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def track_input(self, x: torch.Tensor):\n",
    "\n",
    "        inp = x.detach().float()\n",
    "        inp = rearrange(inp, \"B T H -> (B T) H\")\n",
    "        xtx = inp.T @ inp  # (H, H)\n",
    "\n",
    "        if self.xtx is None:\n",
    "            self.xtx = torch.zeros_like(xtx)\n",
    "\n",
    "        self.xtx = self.xtx + xtx  # sum over iters xTx same as XTX\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.state == ShareLinearState.CALIBRATING:\n",
    "            self.track_input(x)\n",
    "\n",
    "        if self.state != ShareLinearState.COMPRESSED:\n",
    "            return F.linear(x, self.original)\n",
    "\n",
    "        b = F.linear(x, self.basis)\n",
    "        out = F.linear(b, self.coefficient)\n",
    "        return out\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in={self.in_features}, basis={self.basis_features}, out={self.out_features}, state={self.state.value}\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18ee421",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((10, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948a91fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a.T @ a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "import bbml\n",
    "\n",
    "class WeightConfig(BaseModel):\n",
    "    pattern: str\n",
    "    split: bool|Literal[\"qkv\", \"heads\"] = False\n",
    "    qkv_order: str = \"qkv\"\n",
    "    n_head: int|None = None\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def qkv_string(self):\n",
    "        if self.split:\n",
    "            if not self.qkv_order or set(self.qkv_order.lower()) != {\"q\", \"k\", \"v\"}:\n",
    "                raise ValueError(\"qkv_order must contain exactly 'q', 'k', and 'v' characters when split='qkv'\")\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_heads_split(self):\n",
    "        if self.split == \"heads\" and self.n_head is None:\n",
    "            raise ValueError(\"n_head must be set when split='heads'\")\n",
    "        return self\n",
    "\n",
    "\n",
    "Percent = Annotated[float, Field(ge=0, le=1)]\n",
    "class SplitConfig(BaseModel):\n",
    "    weight_types: dict[str, WeightConfig]\n",
    "    group_size: int\n",
    "    layer_pattern: str\n",
    "    compression_rate: Percent\n",
    "\n",
    "\n",
    "class SplitLinearFinetuner(bbml.Finetuner):\n",
    "    def __init__(self, model: bbml.Foundation, config: SplitConfig):\n",
    "        super().__init__(model)\n",
    "        self.config: SplitConfig = config\n",
    "    \n",
    "        self.original_weights = []\n",
    "\n",
    "        named_modules = {k:v for k,v in model.named_modules()}\n",
    "\n",
    "        for name, module in named_modules.items():\n",
    "            for wtype, wtype_cfg in self.config.weight_types.items():\n",
    "                if re.match(wtype_cfg.pattern, name) is not None:\n",
    "                    name_parts = name.split(\".\")\n",
    "                    parent_name = \".\".join(name_parts[:-1])\n",
    "                    parent = named_modules[parent_name]\n",
    "                    list_id = None\n",
    "                    if name_parts[-1].isdigit():\n",
    "                        list_id = int(name_parts[-1])\n",
    "                    \n",
    "                    layer_num = re.search(config.layer_pattern, name).group(1)\n",
    "\n",
    "                    self.original_weights.append({\n",
    "                        \"name\": name,\n",
    "                        \"module\": module,\n",
    "                        \"parent\": parent,\n",
    "                        \"list_id\": list_id,\n",
    "                        \"last_name_part\": name_parts[-1],\n",
    "                        \"layer\": layer_num,\n",
    "                        \"weight_type\": wtype,\n",
    "                    })\n",
    "\n",
    "    def get_train_parameters(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def save(self, save_path: str | Path):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def load(self, load_path: str | Path):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def split_weights(self):\n",
    "\n",
    "        self.all_sharelinears = {}\n",
    "        self.weight_types = defaultdict(list)\n",
    "\n",
    "        for wt_dict in self.original_weights:\n",
    "            wtype = wt_dict[\"weight_type\"]\n",
    "            wtype_cfg = self.config.weight_types[wtype]\n",
    "            module = wt_dict[\"module\"]\n",
    "            name = wt_dict[\"name\"]\n",
    "            has_bias = module.bias is not None\n",
    "            in_feats = module.in_features\n",
    "            out_feats = module.out_features\n",
    "            split_linear = SplitLinear(bias=has_bias, out_features=out_feats)\n",
    "            if has_bias:\n",
    "                split_linear.bias.data = module.bias.data\n",
    "            if wt_dict[\"list_id\"] is None:\n",
    "                setattr(wt_dict[\"parent\"], wt_dict[\"last_name_part\"], split_linear)\n",
    "            else:\n",
    "                idx = wt_dict[\"list_id\"]\n",
    "                wt_dict[\"parent\"][idx] = split_linear\n",
    "\n",
    "            if wtype_cfg.split == \"qkv\" or wtype_cfg.split == \"heads\":\n",
    "                assert out_feats % 3 == 0\n",
    "                qkv_feats = out_feats // 3 \n",
    "            \n",
    "            if wtype_cfg.split == \"qkv\":\n",
    "                cur_ind = 0            \n",
    "                for qkv_part in wtype_cfg.qkv_order:\n",
    "                    to_part = ShareLinear(in_feats, in_feats, qkv_feats)    \n",
    "                    to_part.original.data = module.weight.data[cur_ind:cur_ind+qkv_feats,:]\n",
    "                    cur_ind += qkv_feats\n",
    "                    \n",
    "                    split_linear.splits.append(to_part)\n",
    "                    self.all_sharelinears[f\"{name}.{qkv_part}\"] = to_part\n",
    "                    \n",
    "                    split_wtype = f\"{wtype}.{qkv_part}\"\n",
    "                    self.weight_types[split_wtype].append({\n",
    "                        \"module\": to_part,\n",
    "                        \"name\": f\"{name}.{qkv_part}\",\n",
    "                        \"split_name\": qkv_part,\n",
    "                        \"weight_type\": wtype,\n",
    "                        \"layer\": wt_dict[\"layer\"],\n",
    "                    })\n",
    "                \n",
    "            elif wtype_cfg.split == \"heads\":\n",
    "                n_heads = wtype_cfg.n_head\n",
    "                head_dim = qkv_feats // n_heads\n",
    "\n",
    "                cur_ind = 0\n",
    "                for qkv_part in wtype_cfg.qkv_order:\n",
    "                    for head_num in range(n_heads):\n",
    "                        to_head = ShareLinear(in_feats, in_feats, head_dim)\n",
    "                        to_head.original.data = module.weight.data[cur_ind:cur_ind+head_dim,:]  # [out_dim, in_dim] -> [head_dim, in_dim]\n",
    "                        cur_ind += head_dim\n",
    "\n",
    "                        split_linear.splits.append(to_head)\n",
    "                        self.all_sharelinears[f\"{name}.{qkv_part}.{head_num}\"] = to_head\n",
    "                        \n",
    "                        split_wtype = f\"{wtype}.{qkv_part}.{head_num}\"\n",
    "                        self.weight_types[split_wtype].append({\n",
    "                            \"module\": to_head,\n",
    "                            \"name\": f\"{name}.{qkv_part}.{head_num}\",\n",
    "                            \"split_name\": f\"{qkv_part}.{head_num}\",\n",
    "                            \"weight_type\": wtype,\n",
    "                            \"layer\": wt_dict[\"layer\"],\n",
    "                        })\n",
    "                \n",
    "\n",
    "            else: # no split\n",
    "                in_linear = ShareLinear(in_feats, in_feats, out_feats)\n",
    "                in_linear.original.data = module.weight.data\n",
    "\n",
    "                split_linear.splits.append(in_linear)\n",
    "                self.all_sharelinears[name] = in_linear\n",
    "                self.weight_types[wtype].append({\n",
    "                    \"module\": in_linear,\n",
    "                    \"name\": name,\n",
    "                    \"split_name\": \"\",\n",
    "                    \"weight_type\": wtype,\n",
    "                    \"layer\": wt_dict[\"layer\"],\n",
    "                })\n",
    "        \n",
    "    def group_weights(self):\n",
    "        self.groups = defaultdict(list)\n",
    "        \n",
    "        group_size = self.config.group_size\n",
    "        for weight_type, weights_list in self.weight_types.items():\n",
    "            # group_adjacent\n",
    "            cur_group = []\n",
    "            for weights_dict in weights_list:\n",
    "                cur_group.append(weights_dict)\n",
    "                if len(cur_group) >= group_size:\n",
    "                    self.groups[weight_type].append(cur_group)\n",
    "                    cur_group = []\n",
    "            if len(cur_group) > 0:  # leftovers\n",
    "                self.groups[weight_type].append(cur_group)\n",
    "                cur_group = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def calibrate(self, dataset: Dataset|bbml.DataPipe):\n",
    "        self.model.eval()\n",
    "        for l in self.all_sharelinears.values():\n",
    "            l.state = ShareLinearState.CALIBRATING\n",
    "        \n",
    "        if not isinstance(dataset, bbml.DataPipe):\n",
    "            dataset = bbml.DataPipe(\n",
    "                batch_size=1,\n",
    "                shuffle=False,\n",
    "                num_workers=2,\n",
    "            ).add_dataset(\n",
    "                dataset\n",
    "            ).add_transforms(\n",
    "                self.model.data_transforms\n",
    "            )\n",
    "\n",
    "        dataloader = dataset.get_loader()\n",
    "\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            step_info = {\n",
    "                \"step\": 0,\n",
    "                \"split\": \"validation\",\n",
    "            }\n",
    "            batch.update(step_info)\n",
    "            self.model.single_step(batch)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_num_basis(\n",
    "        in_features: int, out_features: int, compression_ratio: int\n",
    "    ) -> int:\n",
    "        total_original = in_features * out_features\n",
    "        num_basis = (total_original * compression_ratio) / (in_features + out_features)\n",
    "        return max(1, int(num_basis))\n",
    "\n",
    "    def run_svd(self):\n",
    "        self.group_bases = defaultdict(list)\n",
    "        for weight_type, groups_list in self.groups.items():\n",
    "            for group in tqdm.tqdm(groups_list):\n",
    "                all_xtx = []\n",
    "                all_weights = []\n",
    "                out_sizes = []\n",
    "                for weights_dict in group:\n",
    "                    all_xtx.append(weights_dict[\"module\"].xtx)\n",
    "                    all_weights.append(weights_dict[\"module\"].original.data)\n",
    "                    out_sizes.append(weights_dict[\"module\"].original.data.size(1))\n",
    "                    \n",
    "                all_xtx = sum(all_xtx)\n",
    "                try:\n",
    "                    S = torch.linalg.cholesky(all_xtx).T\n",
    "                except Exception as e:\n",
    "                    print(\"Warning: eigen scaling_diag_matrix is not positive!\")\n",
    "                    eigenvalues = torch.linalg.eigvalsh(all_xtx)\n",
    "                    all_xtx += (- eigenvalues[0] + 7e-6) * torch.eye(all_xtx.shape[0]).to(all_xtx.device)\n",
    "                    S = torch.linalg.cholesky(all_xtx).T\n",
    "                S_inv = torch.linalg.inv(S)\n",
    "                W = torch.cat(all_weights, dim=0).T  # -> [in, out_cat]\n",
    "                \n",
    "\n",
    "                W_white = S @ W\n",
    "\n",
    "                U, sigma, Vh = torch.linalg.svd(W_white, full_matrices=False)  # different function torch.svd default some=True is full_matrices=False\n",
    "\n",
    "                total_basis = S_inv @ U @ torch.diag(sigma)\n",
    "                total_coefficient = Vh\n",
    "                compressed_basis = total_basis[:, :k]\n",
    "                compressed_coefficient = total_coefficient[:k, :]\n",
    "\n",
    "\n",
    "                k = self.compute_num_basis(W.size(0), W.size(1), self.config.compression_rate)\n",
    "                group_basis = nn.Parameter(compressed_basis)\n",
    "                self.group_bases[weight_type].append(group_basis)\n",
    "                \n",
    "                cur_id = 0\n",
    "                for i, weights_dict in enumerate(group):\n",
    "                    module = weights_dict[\"module\"]\n",
    "                    module.basis.weight = group_basis\n",
    "                    cur_out_dim = out_sizes[i]\n",
    "                    module.coefficient.weight.data = compressed_coefficient[:,cur_id:cur_id+cur_out_dim]\n",
    "                    cur_id += cur_out_dim\n",
    "                    module.state = ShareLinearState.COMPRESSED\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "691bae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "weight_types:\n",
    "    attn_c_attn:\n",
    "        pattern: '.*\\.attn\\.c_attn$'\n",
    "        split: qkv  # qkv, heads\n",
    "        qkv_order: qkv  # string\n",
    "        n_head: 12\n",
    "    attn_c_proj: \n",
    "        pattern: '.*\\.attn\\.c_proj$'\n",
    "    mlp_c_fc: \n",
    "        pattern: '.*\\.mlp\\.c_fc$'\n",
    "    mlp_c_proj: \n",
    "        pattern: '.*\\.mlp\\.c_proj$'\n",
    "\n",
    "layer_pattern: 'h\\.(\\d+)'\n",
    "\n",
    "group_size: 2\n",
    "compression_rate: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfeb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8286bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec166d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "facf4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitcfg = SplitConfig(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a2521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c0467a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = SplitLinearFinetuner(foundation, splitcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a2edd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.split_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e7c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.group_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb726b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9e0fda427945d7b13a8348bb58da94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c76222fb5734e0ca91fc8705e1b4615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc2bd85da1b416093cdc5ca6d63d678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790c6374cca94f1c800d2752ac3fec0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b899c0cbcedb4df6bbc061d13ef674ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efe49439abd4b8c92fda8322a9c5d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac10dfdb0d0947589b8ab0f962457e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517ac6ecead8476186419ca018c3a696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292c683f7fd2435e8d98cb3725650467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6211 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_dp = DataPipe(\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=16,\n",
    "# ).add_dataset(\n",
    "#     WikiTextDataset(split=\"train\")\n",
    "# ).add_transforms(\n",
    "#     gpt.data_transforms\n",
    "# )\n",
    "from bbml.data.datasets import WikiTextDataset\n",
    "\n",
    "ds = WikiTextDataset(split=\"train\")\n",
    "ds.ds = ds.ds.select(range(256))\n",
    "\n",
    "wrapper.calibrate(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.run_svd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation.run(foundation.input_model(text=\"The quick brown fox jumps over\", max_new_tokens=10)).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.all_basislinears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.weight_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd779363",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(foundation.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(wrapper.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f192f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pprint(wrapper.weight_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb2cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
